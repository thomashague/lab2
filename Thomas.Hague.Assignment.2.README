Thomas Hague, Assignment 2, README

1. I downloaded "A Modest Proposal" and "The Prince."

A Modest Proposal:
original size = 39,698 bytes
compressed size = 27,831 bytes
compression ratio = .70
entropy = 4.64
entropy/8 = .58

The Prince:
original size = 305,864 bytes
compressed size = 218,424 bytes
compression ratio = .714
entropy = 4.49
entropy/8 = .56

2. 1M file
original size = 1,048,576 bytes
compressed size = 0 bytes
compression ratio = 0
entropy = 7.999
entropy/8 = ~1


3. Given the results, it is clear that LZ77 reduces redundancy more effectively when the data has some sort of 
pattern, or is not completely random. 
For the non-text data file, I used this data:
Title:               U.S. / Euro Foreign Exchange Rate
Series ID:           DEXUSEU
Source:              Board of Governors of the Federal Reserve System (US)
Release:             H.10 Foreign Exchange Rates
Seasonal Adjustment: Not Seasonally Adjusted
Frequency:           Daily
Units:               U.S. Dollars to One Euro
Date Range:          1999-01-04 to 2016-02-19
Last Updated:        2016-02-22 3:41 PM CST

This data looks like:
1999-01-04  1.1812
1999-01-05  1.1760
1999-01-06  1.1636
1999-01-07  1.1672
1999-01-08  1.1554
1999-01-11  1.1534
...

original size = 89,400 bytes
compressed size = 30,780 bytes
compression ratio = .344
entropy = 3.63
entropy/8 = .454

This data compresses better than text data. 

4. Analyzing entropy of compressed files:
Compressed Prince entropy = 6.861
Compressed Modest Proposal entropy = 6.862

Both of these are higher than their respective entropy. 
The Zip, gzip, etc. algorithms do a second pass to reduce the redudancy in the encoded files. One main problem with LZ77 is that it often does not bring entropy down to optimal levels. Using another algorithm on top of it, like the Huffman encoding (i.e. DEFLATE) can further reduce the entropy. 

